{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e97706-8608-4b1e-926f-fed953114ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "#Read substructures and datasets\n",
    "datasets = pd.read_excel('ML-improve.xlsx') #Not provided\n",
    "Dipyrone = datasets[datasets['Types of contaminants'] == 'Q']\n",
    "#Dipyrone = datasets[datasets['Type of MB'] == 'RO'] If you want to study RO or NF, remove the previous # and the label above #\n",
    "substructure = pd.read_csv('New Match.txt',header = None)\n",
    "concatenated = pd.concat([Dipyrone.iloc[:, 7:27]], axis=1)\n",
    "arr3 = concatenated.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123d8a8-a464-40f5-a1a2-90d5d343e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match all substructures of SMILES (and record the quantity)\n",
    "arr1 =[[0 for x in range(len(substructure))] for y in range(len(datasets))]\n",
    "smis =datasets.SMILES\n",
    "i = 0\n",
    "j = 0\n",
    "for smi in smis:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    for sub in substructure[0].values :\n",
    "        subMol = Chem.MolFromSmarts(sub)\n",
    "        matches = mol.GetSubstructMatches(subMol)\n",
    "        if len(matches):\n",
    "            arr1[i][j] = len(matches)\n",
    "            if sub =='c': \n",
    "                arr1[i][j] = arr1[i][j]/6\n",
    "        j = j+1\n",
    "    i= i+1\n",
    "    j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b063219-c456-4b70-a3e5-d9327e882c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.hstack((arr1, arr3))\n",
    "\n",
    "\n",
    "X = arr\n",
    "y = Dipyrone.iloc[:, 27]\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#Divide the training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)#37\n",
    "\n",
    "#Initialize the model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "#Set hyperparameter candidate values\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3,4, 5,6,7,8,9,10],\n",
    "    'alpha': [10, 20, 30,40,50,60,70],\n",
    "    'n_estimators': [10,20,30,40 ,50,60,70,80,90,100]\n",
    "}\n",
    "\n",
    "#Create GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='r2', cv=5,n_jobs = -1)\n",
    "\n",
    "#Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Output the optimal hyperparameter combination\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "#Use the model with optimal parameters for prediction\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print('training R2:', metrics.r2_score(y_train,y_train_pred))\n",
    "print('testing R2:', metrics.r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eeeedd-511f-4292-b678-429575545314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the optimal random_state\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#Parameter candidate values - obtained from the above training, the following is a randomly given set of parameters\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.6],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [7],\n",
    "    'alpha': [10],\n",
    "    'n_estimators': [100]\n",
    "}                                                                            \n",
    "\n",
    "#Initialize variables to store the best model and the highest R2 score\n",
    "best_r2 = -np.inf\n",
    "best_model = None\n",
    "best_random_state = None\n",
    "\n",
    "\n",
    "for random_state in range(160):\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "   \n",
    "    model = xgb.XGBRegressor()\n",
    "    \n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='r2', cv=5, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    #If the R2 score of the current model is higher than the previous best score, update the optimal model and score\n",
    "    if test_r2 > 0.8:\n",
    "        best_r2 = test_r2\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_random_state = random_state\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        print('Best random_state:', best_random_state)\n",
    "        print('training R2:', metrics.r2_score(y_train, y_train_pred))\n",
    "        print('test R2:', metrics.r2_score(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# 输出训练和测试 R2 分数\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print('Best random_state:', best_random_state)\n",
    "print('training R2:', metrics.r2_score(y_train, y_train_pred))\n",
    "print('testing R2:', metrics.r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4492285-81fa-4d0e-92f5-9484eda5fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "print('training RMSE:', train_rmse)\n",
    "print('testing RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625261a-2163-4c48-9958-e1d3adc2f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "#Initialize SHAP interpreter using the model with optimal parameters\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "#Calculate SHAP value\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "#Retrieve the name of each row in the substructure and convert it into a list\n",
    "substructure_names = substructure[0].tolist()\n",
    "feature_names = []\n",
    "\n",
    "#Add the name of the original feature\n",
    "for col_name in concatenated.columns:\n",
    "    feature_names.append(col_name)\n",
    "\n",
    "# Add the infrastructure name, and original feature name to feature_name\n",
    "feature_names = substructure_names  + feature_names\n",
    "\n",
    "shap.summary_plot(shap_values, X_train, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af8e30-1a5d-4826-a61b-effc0a052dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "#Calculate SHAP value\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "#Convert to Explanation Object\n",
    "shap_explainer = shap.Explanation(shap_values, feature_names=feature_names)\n",
    "\n",
    "#Draw a bar chart of SHAP feature importance\n",
    "shap.plots.bar(shap_explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb305a-226a-4173-8cc8-e2b99ca9fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average absolute SHAP value for each feature\n",
    "mean_abs_shap = np.abs(shap_explainer.values).mean(axis=0)\n",
    "\n",
    "#Construct a DataFrame containing feature names and corresponding average absolute SHAP values\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Mean Abs SHAP Value': mean_abs_shap\n",
    "})\n",
    "\n",
    "#Export the results as an Excel file\n",
    "importance_df.to_excel(\"shap-DMF-MRL model.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ef549-085b-445a-87fb-78dc5bc809b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Molecular fingerprint analysis\n",
    "sub = shap_values[:,:12]\n",
    "shap.summary_plot(sub, X_train[:, :12],feature_names=substructure_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a5f17-82c3-4a64-bfcb-f397cb77d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# SHAP PDP using C6H6 as an example\n",
    "feature_index = feature_names.index(\"-C6H6\")\n",
    "\n",
    "\n",
    "shap_vals_for_c6h6 = shap_values[:, feature_index]\n",
    "\n",
    "\n",
    "feature_vals_for_c6h6 = X_train[:, feature_index]  # 如果 X_train 是 DataFrame\n",
    "\n",
    "\n",
    "plt.scatter(feature_vals_for_c6h6, shap_vals_for_c6h6, alpha=0.7)\n",
    "plt.xlabel(\"Feature value: -C6H6\")\n",
    "plt.ylabel(\"SHAP value for -C6H6\")\n",
    "plt.title(\"SHAP Dependence for -C6H6\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create DataFrame\n",
    "df_export = pd.DataFrame({\n",
    "    \"feature_value(-C6H6)\": feature_vals_for_c6h6,\n",
    "    \"shap_value(-C6H6)\": shap_vals_for_c6h6\n",
    "})\n",
    "\n",
    "# output Excel \n",
    "output_filename = \"shap-C6H6 scattered data.xlsx\"\n",
    "df_export.to_excel(output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d3386-92ea-46b4-b59b-a5f22c7b2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only export the true and predicted values of the target variable, without including features\n",
    "train_results = pd.DataFrame({\n",
    "    \"y_true\": y_train.values,\n",
    "    \"y_pred\": y_train_pred\n",
    "})\n",
    "test_results = pd.DataFrame({\n",
    "    \"y_true\": y_test.values,\n",
    "    \"y_pred\": y_test_pred\n",
    "})\n",
    "\n",
    "#Use ExcelWriter to save two DataFrames separately to different worksheets\n",
    "with pd.ExcelWriter(\"xgboost.xlsx\") as writer:\n",
    "    train_results.to_excel(writer, sheet_name=\"Train\", index=False)\n",
    "    test_results.to_excel(writer, sheet_name=\"Test\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
